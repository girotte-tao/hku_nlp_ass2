{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-03T07:31:02.067812Z",
     "start_time": "2023-12-03T07:31:00.918264Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (PreTrainedModel, AutoModel,AutoConfig)\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BiLSTM, self).__init__() \n",
    "        #self.setup_seed(seed)\n",
    "        self.forward_lstm = nn.LSTM(hidden_size, hidden_size//2, num_layers=1, bidirectional=False, batch_first=True)\n",
    "        self.backward_lstm = nn.LSTM(hidden_size, hidden_size//2, num_layers=1, bidirectional=False, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size,max_len,feat_dim = x.shape\n",
    "        out1, (h1,c1) = self.forward_lstm(x)\n",
    "        reverse_x = torch.zeros([batch_size, max_len, feat_dim], dtype=torch.float32, device='cuda')\n",
    "        for i in range(max_len):\n",
    "            reverse_x[:,i,:] = x[:,max_len-1-i,:]\n",
    "                \n",
    "        out2, (h2,c2) = self.backward_lstm(reverse_x)\n",
    "\n",
    "        output = torch.cat((out1, out2), 2)\n",
    "        return output,(1,1)\n",
    "\n",
    "\n",
    "class HGNER(nn.Module):\n",
    "    def __init__(self, args, num_labels,hidden_dropout_prob=0.1,windows_list=None):\n",
    "        super(HGNER, self).__init__()\n",
    "\n",
    "\n",
    "        config = AutoConfig.from_pretrained(args.bert_model)\n",
    "        self.bert = AutoModel.from_pretrained(args.bert_model)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "\n",
    "        self.use_bilstm = args.use_bilstm\n",
    "\n",
    "\n",
    "        self.use_multiple_window = args.use_multiple_window\n",
    "        self.windows_list = windows_list\n",
    "        self.connect_type = args.connect_type\n",
    "        connect_type = args.connect_type\n",
    "        self.d_model = args.d_model\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "\n",
    "        if self.use_multiple_window and self.windows_list != None:\n",
    "            if self.use_bilstm:\n",
    "                self.bilstm_layers = nn.ModuleList([BiLSTM(self.d_model) for _ in self.windows_list])\n",
    "\n",
    "            else:\n",
    "                self.bilstm_layers = nn.ModuleList([nn.LSTM(self.d_model, self.d_model, num_layers=1, bidirectional=False, batch_first=True) for _ in self.windows_list])\n",
    "\n",
    "            if connect_type=='dot-att':\n",
    "                self.linear = nn.Linear(self.d_model, self.num_labels)\n",
    "            elif connect_type=='mlp-att':\n",
    "                self.linear = nn.Linear(self.d_model, self.num_labels)\n",
    "                self.Q = nn.Linear(self.d_model * (len(windows_list) + 1), self.d_model)\n",
    "        else:\n",
    "            self.linear = nn.Linear(self.d_model, self.num_labels)\n",
    "\n",
    "\n",
    "    def windows_sequence(self,sequence_output, windows, lstm_layer):\n",
    "        batch_size, max_len, feat_dim = sequence_output.shape\n",
    "        local_final = torch.zeros([batch_size, max_len, feat_dim], dtype=torch.float32, device='cuda')\n",
    "        for i in range(max_len):\n",
    "            index_list = []\n",
    "            for u in range(1, windows // 2 + 1):\n",
    "                if i - u >= 0:\n",
    "                    index_list.append(i - u)\n",
    "                if i + u <= max_len - 1:\n",
    "                    index_list.append(i + u)\n",
    "            index_list.append(i)\n",
    "            index_list.sort()\n",
    "            temp = sequence_output[:, index_list, :]\n",
    "            out,(h,b) = lstm_layer(temp)\n",
    "            local_f = out[:, -1, :]\n",
    "            local_final[:, i, :] = local_f\n",
    "        return local_final\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,valid_ids=None,attention_mask_label=None):\n",
    "\n",
    "        sequence_output = self.bert(input_ids, token_type_ids= token_type_ids, attention_mask=attention_mask,head_mask=None)[0]\n",
    "        batch_size,max_len,feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda')\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            jj = -1\n",
    "            for j in range(max_len):\n",
    "                    if valid_ids[i][j].item() == 1:\n",
    "                        jj += 1\n",
    "                        valid_output[i][jj] = sequence_output[i][j]\n",
    "        sequence_output = self.dropout(valid_output)\n",
    "\n",
    "\n",
    "        if self.use_multiple_window:\n",
    "            mutiple_windows = []\n",
    "\n",
    "            for i,window in enumerate(self.windows_list):\n",
    "                if self.use_bilstm:\n",
    "                    local_final = self.windows_sequence(sequence_output, window, self.bilstm_layers[i])\n",
    "                mutiple_windows.append(local_final)\n",
    "\n",
    "\n",
    "            if self.connect_type=='dot-att':\n",
    "                muti_local_features = torch.stack(mutiple_windows, dim=2)\n",
    "                sequence_output = sequence_output.unsqueeze(dim=2)\n",
    "                d_k = sequence_output.size(-1)\n",
    "                attn = torch.matmul(sequence_output, muti_local_features.permute(0, 1, 3, 2)) / math.sqrt(d_k)\n",
    "                attn = torch.softmax(attn, dim=-1)\n",
    "                local_features = torch.matmul(attn, muti_local_features).squeeze()\n",
    "                sequence_output = sequence_output.squeeze()\n",
    "                sequence_output = sequence_output + local_features\n",
    "            elif self.connect_type == 'mlp-att':\n",
    "                mutiple_windows.append(sequence_output)\n",
    "                muti_features = torch.cat(mutiple_windows, dim=-1)\n",
    "                muti_local_features = torch.stack(mutiple_windows, dim=2)\n",
    "                query = self.Q(muti_features)\n",
    "                d_k = query.size(-1)\n",
    "                query = query.unsqueeze(dim=2)\n",
    "                attn = torch.matmul(query, muti_local_features.permute(0, 1, 3, 2)) / math.sqrt(d_k)\n",
    "                attn = torch.softmax(attn, dim=-1)\n",
    "                sequence_output = torch.matmul(attn, muti_local_features).squeeze()\n",
    "\n",
    "\n",
    "        logits = self.linear(sequence_output)\n",
    "        \n",
    "        if labels is not None:\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n",
    "            # Only keep active parts of the loss\n",
    "            #attention_mask_label = None\n",
    "            if attention_mask_label is not None:\n",
    "                active_loss = attention_mask_label.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            \n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "#import truecase\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# from pytorch_transformers import (WEIGHTS_NAME, AdamW, BertConfig,\n",
    "#                                   BertForTokenClassification, BertTokenizer,\n",
    "#                                   WarmupLinearSchedule)\n",
    "from  transformers import WEIGHTS_NAME, AdamW, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import (PreTrainedModel, AutoModel,AutoConfig)\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d18a1a14a9407164"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
