# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EPRWWTnclIZrQ8Sq9f8Rp3bUjz5Ct_RV
"""

import torch
from torch import nn, optim
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.functional import to_map_style_dataset
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('runs/lstm_tagger_experiment_1')

# Load dataset
def load_data(file_path):
    """
      a sentence is the first word of each line before seeing a blank line
      same for label but is the forth word
    """
    with open(file_path, encoding='utf-8') as file:
        lines = file.readlines()

    sentences, labels = [], []
    sentence, sentence_labels = [], []
    for line in lines:
        if line.startswith("-DOCSTART-") or line == "\n":
            if sentence:
                sentences.append(sentence)
                labels.append(sentence_labels)
                sentence, sentence_labels = [], []
        else:
            parts = line.split()
            sentence.append(parts[0])
            sentence_labels.append(parts[-1])

    return sentences, labels

def build_vocab(data_iter):
    specials = ['<unk>', '<pad>']
    vocab = build_vocab_from_iterator(data_iter, specials=specials)
    vocab.set_default_index(vocab['<unk>'])
    return vocab


def process_data(sentences, labels, word_vocab, label_vocab):
    processed_sentences = [torch.tensor([word_vocab[word] for word in sentence], dtype=torch.long) for sentence in sentences]
    processed_labels = [torch.tensor([label_vocab[label] for label in label], dtype=torch.long) for label in labels]

    return list(zip(processed_sentences, processed_labels))

def collate_batch(batch):
    text_list, label_list = zip(*batch)
    text_list = pad_sequence(text_list, padding_value=word_vocab['<pad>'])
    label_list = pad_sequence(label_list, padding_value=label_vocab['<pad>'])
    return text_list, label_list

# load dataset
sentences, labels = load_data("conll2003/train.txt")
word_vocab = build_vocab(sentences)
label_vocab = build_vocab(labels)
processed_data = process_data(sentences, labels, word_vocab, label_vocab)
train_dataset = to_map_style_dataset(processed_data)

# define hyperparameters
# BATCH_SIZE = 64
# LEARNING_RATE = 0.0003
# INPUT_DIM = len(word_vocab)
# EMBEDDING_DIM = 100
# HIDDEN_DIM = 256
# OUTPUT_DIM = len(label_vocab)
# NUM_LAYERS = 4
# DROPOUT_RATE = 0.2

# define hyperparameters
BATCH_SIZE = 32
LEARNING_RATE = 0.1
INPUT_DIM = len(word_vocab)
EMBEDDING_DIM = 64
HIDDEN_DIM = 128
OUTPUT_DIM = len(label_vocab)
NUM_LAYERS = 4
DROPOUT_RATE = 0.1


train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)

class LSTMTagger(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers, dropout, bidirectional=False):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)
        self.fc = nn.Linear(hidden_dim *2, output_dim) if bidirectional else nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.embedding(text)
        outputs, (hidden, _) = self.lstm(embedded)
        predictions = self.fc(outputs)
        return predictions

# whether bidirectional is set below
model = LSTMTagger(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, DROPOUT_RATE, bidirectional=True)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=label_vocab['<pad>'])

valid_sentences, valid_labels = load_data("conll2003/valid.txt")
processed_valid_data = process_data(valid_sentences, valid_labels, word_vocab, label_vocab)
valid_dataset = to_map_style_dataset(processed_valid_data)

valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)

def evaluate(model, iterator, criterion):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for text, labels in iterator:
            text, labels = text.to(device), labels.to(device)

            predictions = model(text)
            predictions = predictions.view(-1, predictions.shape[-1])
            labels = labels.view(-1)
            loss = criterion(predictions, labels)
            total_loss += loss.item()

    return total_loss / len(iterator)

def train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, n_epochs, device):
    for epoch in range(n_epochs):
        model.train()
        total_train_loss = 0

        for text, labels in train_loader:
            text, labels = text.to(device), labels.to(device)

            optimizer.zero_grad()
            predictions = model(text)
            predictions = predictions.view(-1, predictions.shape[-1])
            labels = labels.view(-1)

            loss = criterion(predictions, labels)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()

        train_loss = total_train_loss / len(train_loader)
        eval_loss = evaluate(model, valid_loader, criterion)

        writer.add_scalar('Loss/Train', train_loss, epoch)
        writer.add_scalar('Loss/Validation', eval_loss, epoch)

        print(f'Epoch: {epoch+1:02}')
        print(f'\tTrain Loss: {train_loss:.3f}')
        print(f'\t Eval Loss: {eval_loss:.3f}')

N_EPOCHS = 50
train_and_evaluate(model, train_loader, valid_loader, optimizer, criterion, N_EPOCHS, device)

writer.close()